Experiments:


today:
 * abb robot API
 * run once on real robot
 * incorporate pseudo reward for endeffector vs. cube
 * vision


this week:
 * ping pong in simulator
 * new RL algorithm


Ideas:
 * incorporate sin(), cos() of orientations


Lessons:
 * fast critic learning is helpful
 * normalization of inputs / batchnorm didn't work
 * fixed_kl works great
 * a100: more accurate end position


Papers:
 * deep mind paper: emergence of locomotion paper
 * paper: deep rl that matters


Questions:
 * 3000 simulation step / 10 skipframe = 300 timestep
 * 4 layer
 * use var to directly output stdev?
 * how is std dev variable updated?
 * performance degrades. e.g. ppo_push_obj_rew_a100. perf works around 30th video. ~1 stdev
 * RNN?