Experiments results:
 * two_layer_a100: didn't work
 * two_layer: ent_reg=0.001. worked. peak reward 12
 * three_layer: ent_reg=0.001. worked. peak reward 19
 * fast_critic: a100 worked. peak reward 19


Lessons:
 * normalization of inputs / batchnorm didn't work


Ideas:
 * try set initial stdev to higher
 * batchnorm: encourage exploration
 * 2 layer: encourage exploration
 * 3 layer use ent reg
 * std_fc: promising. entropy keeps rising. use 3 layer
 * use running mean for observation
 * make critic learn 10x faster actor


Papers:
 * deep mind paper: emergence of locomotion paper
 * paper: deep rl that matters


Questions:
 * 3000 simulation step / 10 skipframe = 300 timestep
 * 4 layer
 * use var to directly output stdev?
 * how is std dev variable updated?
 * performance degrades. e.g. ppo_push_obj_rew_a100. perf works around 30th video. ~1 stdev
 * RNN?