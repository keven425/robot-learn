Experiments:
 * actor_dist: promising
 * cv_pos: doesn't work
 * vel_force_ctrl: only learns approaching obj. doesn't learn how to move
 * pingpong: frameskip 1 not learning. frameskip 10 ball sticks on pedal eventually


this week:
 * debug vary start/end: feeding position correctly?
 * abb robot API
 * run once on real robot
 * vision: try new experiments. spatial softmax. multiple cameras.
 * ping pong in simulator
 * variable start/end position
 * new RL algorithm


Ideas:
 * incorporate sin(), cos() of orientations
 * forward kinematics to limit endeffector positions within bounding box
 * pingpong: always use frameskip 1. more realistic
 * cv: input diff b/t goal & current image
 * vary: fix position in a given batch
 * cv: predict endeffector position


Lessons:
 * fast critic learning is helpful
 * normalization of inputs / batchnorm didn't work
 * fixed_kl works great
 * a100: more accurate end position


Papers:
 * deep mind paper: emergence of locomotion paper
 * paper: deep rl that matters


Questions:
 * 3000 simulation step / 10 skipframe = 300 timestep
 * 4 layer
 * use var to directly output stdev?
 * how is std dev variable updated?
 * performance degrades. e.g. ppo_push_obj_rew_a100. perf works around 30th video. ~1 stdev
 * RNN?